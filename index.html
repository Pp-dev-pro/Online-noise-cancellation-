<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>RNNoise Web Demo — Enhanced UI</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
<style>
  :root{
    --bg:#061017; --card:rgba(255,255,255,0.03); --muted:#9fb3c6; --accent:#4bd3ff;
  }
  html,body{height:100%; margin:0; font-family:Inter,system-ui,Roboto,Arial; background:
    radial-gradient(800px 400px at 10% 10%, rgba(75,211,255,0.02), transparent 10%),
    linear-gradient(180deg,#04101a 0%, #06131a 100%); color:#e8f6fb;}
  .wrap{max-width:1100px;margin:28px auto;padding:22px;}
  header{display:flex;gap:16px;align-items:center}
  h1{margin:0;font-size:1.4rem}
  p.lead{margin:0;color:var(--muted);font-size:0.95rem}

  .controls{display:flex;gap:10px;flex-wrap:wrap;margin-top:16px;align-items:center}
  .btn{background:linear-gradient(180deg,#0f2130,#071218);border:1px solid rgba(255,255,255,0.04);
    color:#dff7ff;padding:10px 14px;border-radius:10px;cursor:pointer;box-shadow:0 8px 24px rgba(0,0,0,0.6);}
  .btn.primary{background:linear-gradient(180deg,#06b6ff,#0284c7); color:#012; font-weight:700;}
  .btn.ghost{background:transparent;border:1px dashed rgba(255,255,255,0.03);color:var(--muted);}
  .btn:disabled{opacity:0.45;cursor:not-allowed}

  .card{background:var(--card);border-radius:12px;padding:14px;margin-top:14px;backdrop-filter: blur(8px) saturate(120%);
    border:1px solid rgba(255,255,255,0.03);box-shadow:0 10px 30px rgba(0,0,0,0.6);transition:transform .25s}
  .card:hover{transform:translateY(-6px)}

  .grid{display:grid;grid-template-columns:1fr 360px;gap:14px;margin-top:14px;}
  @media (max-width:880px){ .grid{grid-template-columns:1fr} }

  canvas{width:100%;height:160px;background:#000;border-radius:8px;display:block}
  .meta{color:var(--muted);font-size:0.9rem;margin-top:8px}
  label.small{font-size:0.9rem;color:var(--muted);display:flex;gap:8px;align-items:center}
  input[type=range]{accent-color:var(--accent)}
  footer{margin-top:18px;color:var(--muted);font-size:0.85rem}
  .status { font-weight:600; color:#bfefff; }
  .mute-warning { color:#ffb4b4; font-weight:600; margin-top:8px;}
  .controls .spacer {flex:1}
  .link{color:var(--accent); text-decoration:none;}
</style>
</head>
<body>
  <div class="wrap">
    <header>
      <div>
        <h1>RNNoise Web Demo — Enhanced UI</h1>
        <p class="lead">WebAssembly RNNoise + polished UI. Uses RNNoise WASM to denoise microphone input in real-time (demo). Use headphones.</p>
      </div>
      <div style="margin-left:auto;text-align:right">
        <div class="meta">Frame size: <strong>480 samples</strong> · Source: <a class="link" href="https://github.com/xiph/rnnoise" target="_blank">xiph/rnnoise</a></div>
        <div class="meta">WASM build: <a class="link" href="https://github.com/shiguredo/rnnoise-wasm" target="_blank">@shiguredo/rnnoise-wasm</a></div>
      </div>
    </header>

    <div class="controls">
      <button id="startBtn" class="btn primary">Start</button>
      <button id="stopBtn" class="btn" disabled>Stop</button>
      <button id="toggleDenoiseBtn" class="btn ghost" disabled>Enable Denoise</button>

      <div class="spacer"></div>

      <label class="small">Gain <input id="gain" type="range" min="0" max="2" step="0.01" value="1"></label>
      <label class="small">Mix <input id="mix" type="range" min="0" max="1" step="0.01" value="0"></label>
    </div>

    <div class="grid">
      <div>
        <div class="card">
          <h3 style="margin:0 0 8px 0">Waveform (input)</h3>
          <canvas id="wave"></canvas>
          <div class="meta" id="statusText">status: <span class="status">idle</span></div>
        </div>

        <div class="card" style="margin-top:12px">
          <h3 style="margin:0 0 8px 0">Spectrum (input)</h3>
          <canvas id="spectrum"></canvas>
          <div class="meta" id="peaks">Detected peaks: —</div>
        </div>
      </div>

      <div>
        <div class="card">
          <h3 style="margin:0 0 8px 0">Controls & Info</h3>
          <div class="meta">RNNoise WASM loaded: <span id="wasmLoaded">no</span></div>
          <div class="meta">Denoiser state: <span id="denoiseState">none</span></div>
          <div class="meta" style="margin-top:8px">Latency: <span id="latency">—</span></div>
          <div class="mute-warning" id="warn">Start the demo and use headphones. Keep volume low initially.</div>
        </div>

        <div class="card" style="margin-top:12px">
          <h3 style="margin:0 0 8px 0">About / Notes</h3>
          <div class="meta">This demo demonstrates RNNoise running in the browser (WASM). For production-grade low-latency integrate RNNoise inside an AudioWorklet with a synchronous WASM build (see shiguredo/jitsi notes).</div>
          <div class="meta" style="margin-top:8px"><a class="link" href="https://github.com/shiguredo/rnnoise-wasm" target="_blank">@shiguredo/rnnoise-wasm</a> · <a class="link" href="https://github.com/xiph/rnnoise" target="_blank">xiph/rnnoise</a></div>
        </div>
      </div>
    </div>

    <footer>Serve over <strong>HTTPS</strong> (GitHub Pages). Denoise switch toggles RNNoise processing; Stop now reliably disconnects audio.</footer>
  </div>

<script type="module">
/*
  RNNoise Web demo (script-processor circular-buffer approach).
  - Requires: RNNoise WASM module (shiguredo / jitsi builds)
  - Behavior: captures mic -> buffers -> when 480 samples available -> calls rnnoise.processFrame -> outputs denoised audio
  - Note: ScriptProcessor is used here for simplicity and compatibility; for best latency use AudioWorklet + sync-WASM (see README of rnnoise-wasm).
*/

/* ---------------------------
   CONFIG: CDN path to rnnoise-wasm
   (replace version if you want a different one)
   --------------------------- */
const RNNOISE_CDN = 'https://cdn.jsdelivr.net/npm/@shiguredo/rnnoise-wasm@2025.1.5/dist/index.js';

const startBtn = document.getElementById('startBtn');
const stopBtn  = document.getElementById('stopBtn');
const toggleDenoiseBtn = document.getElementById('toggleDenoiseBtn');
const gainEl = document.getElementById('gain');
const mixEl  = document.getElementById('mix');
const statusText = document.getElementById('statusText');
const wasmLoadedEl = document.getElementById('wasmLoaded');
const denoiseStateEl = document.getElementById('denoiseState');
const peaksEl = document.getElementById('peaks');
const latencyEl = document.getElementById('latency');

const waveCanvas = document.getElementById('wave'), waveCtx = waveCanvas.getContext('2d');
const specCanvas = document.getElementById('spectrum'), specCtx = specCanvas.getContext('2d');

let audioCtx = null, micStream = null, sourceNode = null;
let scriptNode = null, gainNode = null, analyser = null, freqAnalyser = null;
let rn = null, denoiser = null;
let denoiseEnabled = false;

// Circular buffer for RNNoise frames
const FRAME_SIZE = 480; // RNNoise expects 480-sample frames
let circBuf = null, circWrite = 0, circRead = 0, circLen = 0;

// Visualizer utilities (resize for DPR)
function resizeCanvasToDisplaySize(canvas){
  const rect = canvas.getBoundingClientRect();
  const dpr = window.devicePixelRatio || 1;
  canvas.width = Math.round(rect.width * dpr);
  canvas.height = Math.round(rect.height * dpr);
  return {w:canvas.width, h:canvas.height, dpr};
}

function draw(){
  if(!analyser || !audioCtx) { requestAnimationFrame(draw); return; }
  requestAnimationFrame(draw);

  // waveform
  const timeData = new Uint8Array(analyser.fftSize);
  analyser.getByteTimeDomainData(timeData);
  const {w:ww, h:wh} = resizeCanvasToDisplaySize(waveCanvas);
  waveCtx.fillStyle = '#000'; waveCtx.fillRect(0,0,ww,wh);
  waveCtx.lineWidth = 2 * (window.devicePixelRatio||1);
  waveCtx.strokeStyle = '#4ee7c6'; waveCtx.beginPath();
  for(let i=0;i<timeData.length;i++){
    const x = (i / timeData.length) * ww;
    const y = (timeData[i]/255) * wh;
    if(i===0) waveCtx.moveTo(x,y); else waveCtx.lineTo(x,y);
  }
  waveCtx.stroke();

  // spectrum
  const specData = new Uint8Array(freqAnalyser.frequencyBinCount);
  freqAnalyser.getByteFrequencyData(specData);
  const {w:fw, h:fh} = resizeCanvasToDisplaySize(specCanvas);
  specCtx.clearRect(0,0,fw,fh);
  const barW = fw / specData.length;
  // detect top peaks (for simple display)
  let top = [];
  for(let i=0;i<specData.length;i++){
    if(specData[i] > 200) top.push({i, v:specData[i]});
  }
  peaksEl.textContent = top.length ? top.slice(0,6).map(p=>Math.round((p.i * audioCtx.sampleRate) / (specData.length*2))+'Hz').join(', ') : '—';
  for(let i=0;i<specData.length;i++){
    const v = specData[i]/255;
    specCtx.fillStyle = `hsl(${Math.round((i/specData.length)*280)},80%,60%)`;
    specCtx.fillRect(i*barW, fh - v*fh, barW*0.9, v*fh);
  }
}

// Load RNNoise WASM module from CDN (dynamic import ESM)
async function loadRnnoise(){
  try{
    // dynamic import; package exposes Rnnoise
    const mod = await import(RNNOISE_CDN);
    // some packages export Rnnoise named export, some default; check both
    const Rnnoise = mod?.Rnnoise ?? mod?.default?.Rnnoise ?? mod?.default;
    if(!Rnnoise) throw new Error('RNNoise module not found in CDN bundle');
    const loaded = await Rnnoise.load(); // returns API object
    return loaded;
  }catch(err){
    console.error('Failed to load RNNoise WASM from CDN', err);
    throw err;
  }
}

// Start: capture mic and start processing
async function start(){
  if(audioCtx) return;
  statusText.querySelector('.status').textContent = 'starting...';
  try{
    micStream = await navigator.mediaDevices.getUserMedia({ audio: { channelCount:1, echoCancellation:false, noiseSuppression:false, autoGainControl:false } });
  }catch(e){
    statusText.querySelector('.status').textContent = 'mic permission denied';
    console.error(e); return;
  }

  audioCtx = new (window.AudioContext || window.webkitAudioContext)({ latencyHint:'interactive' });
  sourceNode = audioCtx.createMediaStreamSource(micStream);

  // analyzers for visuals
  analyser = audioCtx.createAnalyser(); analyser.fftSize = 2048;
  freqAnalyser = audioCtx.createAnalyser(); freqAnalyser.fftSize = 4096;
  sourceNode.connect(analyser); sourceNode.connect(freqAnalyser);

  // script processor for processing & output (use small buffer for lower latency)
  const bufferSize = 128; // keep low; RNNoise frames are buffered separately
  scriptNode = audioCtx.createScriptProcessor(bufferSize, 1, 1);

  // circular buffer sized to hold multiple frames (e.g., 8*FRAME_SIZE)
  const cbCapacity = FRAME_SIZE * 8;
  circBuf = new Float32Array(cbCapacity);
  circWrite = 0; circRead = 0; circLen = 0;

  // lazy load rnnoise WASM
  if(!rn){
    try{
      rn = await loadRnnoise();
      wasmLoadedEl.textContent = 'yes';
    }catch(e){
      wasmLoadedEl.textContent = 'failed';
      console.error(e);
      // allow continued operation without rnnoise (so user can test waveform)
    }
  }

  // create denoiser state if we have rn
  if(rn && !denoiser){
    denoiser = rn.createDenoiseState();
    denoiseStateEl.textContent = 'created';
    toggleDenoiseBtn.disabled = false;
    // default enabled
    denoiseEnabled = true;
    toggleDenoiseBtn.textContent = 'Disable Denoise';
  }

  // gain node
  gainNode = audioCtx.createGain();
  gainNode.gain.value = parseFloat(gainEl.value);

  // processing function: feed samples to circ buffer; when we have full frames, call processFrame
  scriptNode.onaudioprocess = function(evt){
    const input = evt.inputBuffer.getChannelData(0);
    const output = evt.outputBuffer.getChannelData(0);

    // write input to circular buffer
    for(let i=0;i<input.length;i++){
      circBuf[circWrite] = input[i];
      circWrite = (circWrite + 1) % circBuf.length;
      if(circLen < circBuf.length) circLen++; else circRead = (circRead + 1) % circBuf.length; // overwrite older if full
    }

    // Prepare an output buffer to fill (we produce processed samples if available; else passthrough)
    let outIdx = 0;
    while(outIdx < output.length){
      // If we have at least FRAME_SIZE samples available, process next chunk(s)
      if(denoiseEnabled && denoiser && circLen >= FRAME_SIZE){
        // build a contiguous Float32Array of FRAME_SIZE (handle wrap)
        const frame = new Float32Array(FRAME_SIZE);
        // copy from circRead
        for(let k=0;k<FRAME_SIZE;k++){
          frame[k] = circBuf[(circRead + k) % circBuf.length];
        }
        // process in place (rnnoise API mutates the frame)
        try{
          denoiser.processFrame(frame);
        }catch(e){
          console.warn('rnnoise processFrame error', e);
        }
        // advance read index by FRAME_SIZE
        circRead = (circRead + FRAME_SIZE) % circBuf.length;
        circLen -= FRAME_SIZE;

        // now write results to output (mix according to mix slider)
        const mix = parseFloat(mixEl.value);
        for(let j=0;j<FRAME_SIZE && outIdx < output.length; j++){
          // when mixing with original input we need original sample; we don't have it now; use processed sample directly
          output[outIdx++] = frame[j] * (1 - mix) + 0 * mix; // passthrough portion is 0 here (we could keep separate raw buffer if needed)
        }
      } else {
        // not enough processed samples: fall back to pass-through latest input samples (best-effort)
        // read from a local pointer (we'll output zeros to avoid feedback; safer for headphones)
        const fallback = 0.0;
        output[outIdx++] = fallback;
      }
    }
  };

  // connect graph: source -> scriptNode -> gain -> dest
  sourceNode.connect(scriptNode);
  scriptNode.connect(gainNode);
  gainNode.connect(audioCtx.destination);

  // UI updates
  startBtn.disabled = true; stopBtn.disabled = false;
  statusText.querySelector('.status').textContent = 'running';
  latencyEl.textContent = ((audioCtx.baseLatency || 0) * 1000).toFixed(1) + ' ms';
  requestAnimationFrame(draw);
}

// Stop and cleanup (fixed: fully removes nodes, stops tracks, frees denoiser)
function stop(){
  if(!audioCtx) return;
  // disconnect nodes first
  try{ if(sourceNode) sourceNode.disconnect(); }catch(e){}
  try{ if(scriptNode) scriptNode.disconnect(); }catch(e){}
  try{ if(gainNode) gainNode.disconnect(); }catch(e){}
  // stop mic tracks
  try{ if(micStream) micStream.getTracks().forEach(t=>t.stop()); }catch(e){}
  // destroy denoiser state to free WASM memory if present
  try{ if(denoiser){ denoiser.destroy(); denoiser = null; denoiseStateEl.textContent = 'destroyed'; } }catch(e){ console.warn(e); }
  // close audio context
  try{ audioCtx.close(); }catch(e){}
  // reset local refs
  audioCtx = null; micStream = null; sourceNode = null; scriptNode = null; gainNode = null; analyser = null; freqAnalyser = null;
  circBuf = null; circLen = circRead = circWrite = 0;
  startBtn.disabled = false; stopBtn.disabled = true; toggleDenoiseBtn.disabled = true;
  statusText.querySelector('.status').textContent = 'stopped';
  latencyEl.textContent = '—';
}

// Toggle denoise on/off (keeps denoiser instance)
function toggleDenoise(){
  denoiseEnabled = !denoiseEnabled;
  toggleDenoiseBtn.textContent = denoiseEnabled ? 'Disable Denoise' : 'Enable Denoise';
  statusText.querySelector('.status').textContent = denoiseEnabled ? 'denoising' : 'passthrough';
}

// UI wiring
startBtn.addEventListener('click', start);
stopBtn.addEventListener('click', stop);
toggleDenoiseBtn.addEventListener('click', toggleDenoise);
gainEl.addEventListener('input', ()=> { if(gainNode) gainNode.gain.value = parseFloat(gainEl.value); });
mixEl.addEventListener('input', ()=> { /* mix used in processing */ });

// initial canvas sizing
function initialResize(){ resizeCanvasToDisplaySize(waveCanvas); resizeCanvasToDisplaySize(specCanvas); }
window.addEventListener('resize', initialResize); initialResize();

</script>
</body>
          </html>
