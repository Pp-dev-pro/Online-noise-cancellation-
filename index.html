<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Web Anti-Noise Demo</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body { font-family: system-ui, -apple-system, "Segoe UI", Roboto, Arial; max-width:800px; margin:32px auto; padding:0 16px; color:#111; }
    h1 { margin:0 0 8px 0; font-size:1.6rem; }
    p { margin:8px 0 16px 0; color:#333; }
    .controls { display:flex; gap:8px; flex-wrap:wrap; align-items:center; margin-bottom:16px; }
    button { padding:8px 12px; border-radius:8px; border:1px solid #ccc; background:#f6f6f6; cursor:pointer; }
    button.primary { background:#111; color:#fff; border-color:#111; }
    label { display:flex; gap:8px; align-items:center; }
    input[type=range]{ width:180px; }
    pre { background:#f3f3f3; padding:12px; border-radius:6px; overflow:auto; }
    .warn { color:#a33; font-weight:600; }
    .info { color:#0a66c2; }
  </style>
</head>
<body>
  <h1>Web Anti-Noise (Real-time Inversion)</h1>
  <p>
    Captures microphone audio, inverts the waveform, and plays anti-noise back.
    <span class="warn">Use headphones</span>. For best results: low-frequency steady noise, low latency, and disable browser echo-cancellation.
  </p>

  <div class="controls">
    <button id="startBtn" class="primary">Start</button>
    <button id="stopBtn" disabled>Stop</button>

    <label title="Master output volume">
      Output volume
      <input id="gainSlider" type="range" min="0" max="1" step="0.01" value="1">
    </label>

    <label title="Optional feedforward mix (0 = pure anti-noise, 1 = passthrough)">
      Mix
      <input id="mixSlider" type="range" min="0" max="1" step="0.01" value="0.0">
    </label>

    <div style="margin-left:auto; text-align:right;">
      <div>AudioWorklet: <span id="workletSupport">checking…</span></div>
      <div>Latency (estimated): <span id="latency">—</span></div>
    </div>
  </div>

  <p class="info">
    Tips: allow mic, use headphones, reduce system audio effects. If you hear echo or feedback, stop and unplug speakers.
  </p>

  <pre id="log">Console output will appear here.</pre>

<script>
(async function () {
  const startBtn = document.getElementById('startBtn');
  const stopBtn  = document.getElementById('stopBtn');
  const gainSlider = document.getElementById('gainSlider');
  const mixSlider = document.getElementById('mixSlider');
  const logEl = document.getElementById('log');
  const workletSupportEl = document.getElementById('workletSupport');
  const latencyEl = document.getElementById('latency');

  function log(...args) {
    console.log(...args);
    logEl.textContent += args.map(a => typeof a==='object' ? JSON.stringify(a) : String(a)).join(' ') + "\n";
    logEl.scrollTop = logEl.scrollHeight;
  }

  let audioContext = null;
  let micStream = null;
  let sourceNode = null;
  let inverterNode = null;
  let gainNode = null;
  let running = false;
  let useWorklet = !!(window.AudioWorkletNode && AudioWorkletProcessor);

  workletSupportEl.textContent = useWorklet ? "yes" : "no (will try ScriptProcessor fallback)";

  // AudioWorklet processor code as a string (we'll make a blob)
  const inverterWorkletCode = `
    class InverterProcessor extends AudioWorkletProcessor {
      static get parameterDescriptors() {
        return [
          { name: 'mix', defaultValue: 0.0, minValue:0, maxValue:1 }, // 0=pure anti-noise, 1=passthrough
          { name: 'gain', defaultValue: 1.0, minValue:0, maxValue:2 }
        ];
      }
      constructor(options) {
        super();
      }
      process (inputs, outputs, parameters) {
        const input = inputs[0];
        const output = outputs[0];
        if (!input || input.length === 0) {
          // silence if no input
          return true;
        }
        const mixArr = parameters.mix;
        const gainArr = parameters.gain;

        for (let channel = 0; channel < output.length; ++channel) {
          const inchan = input[channel] || input[0]; // fallback if mono
          const outchan = output[channel];
          if (!inchan) continue;

          for (let i = 0; i < outchan.length; ++i) {
            // parameter arrays are either length 1 (k-rate) or 128 (a-rate)
            const mix = mixArr.length > 1 ? mixArr[i] : mixArr[0];
            const gain = gainArr.length > 1 ? gainArr[i] : gainArr[0];

            // anti-noise: invert the input sample, scale by gain
            const anti = -inchan[i] * gain;

            // mix between anti-noise and passthrough input:
            outchan[i] = anti * (1 - mix) + inchan[i] * mix;
          }
        }
        return true;
      }
    }
    registerProcessor('inverter-processor', InverterProcessor);
  `;

  // Fallback ScriptProcessor implementation (higher latency)
  function createScriptProcessorInverter(context, bufferSize = 1024) {
    const sp = context.createScriptProcessor(bufferSize, 1, 1);
    sp.onaudioprocess = (evt) => {
      const inbuf = evt.inputBuffer;
      const outbuf = evt.outputBuffer;
      const inData = inbuf.getChannelData(0);
      const outData = outbuf.getChannelData(0);
      const gain = parseFloat(gainSlider.value);
      const mix = parseFloat(mixSlider.value);
      for (let i = 0; i < inData.length; ++i) {
        const anti = -inData[i] * gain;
        outData[i] = anti * (1 - mix) + inData[i] * mix;
      }
    };
    return sp;
  }

  async function start() {
    if (running) return;
    log("Starting...");
    try {
      // Request microphone with raw audio (disable browser processing)
      micStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 44100,
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false
        }
      });
    } catch (err) {
      log("getUserMedia error:", err.message || err);
      return;
    }

    // Create/resume AudioContext
    audioContext = new (window.AudioContext || window.webkitAudioContext)({ latencyHint: 'interactive' });
    await audioContext.audioWorklet?.readyCatch?.(); // no-op but nice for some browsers
    sourceNode = audioContext.createMediaStreamSource(micStream);

    // Create gain node (master output)
    gainNode = audioContext.createGain();
    gainNode.gain.value = parseFloat(gainSlider.value);

    if (useWorklet) {
      // Register worklet
      const blob = new Blob([inverterWorkletCode], { type: 'application/javascript' });
      const blobUrl = URL.createObjectURL(blob);
      try {
        await audioContext.audioWorklet.addModule(blobUrl);
        inverterNode = new AudioWorkletNode(audioContext, 'inverter-processor', {
          numberOfInputs: 1,
          numberOfOutputs: 1,
          channelCount: 1,
          parameterData: { mix: parseFloat(mixSlider.value), gain: parseFloat(gainSlider.value) }
        });

        // Keep parameters in sync with sliders
        mixSlider.addEventListener('input', () => {
          inverterNode.parameters.get('mix').setValueAtTime(parseFloat(mixSlider.value), audioContext.currentTime);
        });
        gainSlider.addEventListener('input', () => {
          inverterNode.parameters.get('gain').setValueAtTime(parseFloat(gainSlider.value), audioContext.currentTime);
          gainNode.gain.setValueAtTime(parseFloat(gainSlider.value), audioContext.currentTime);
        });

        sourceNode.connect(inverterNode);
        inverterNode.connect(gainNode);
        gainNode.connect(audioContext.destination);

        log("AudioWorklet inverter node running.");
      } catch (err) {
        log("AudioWorklet failed — falling back to ScriptProcessor:", err.message || err);
        inverterNode = createScriptProcessorInverter(audioContext, 1024);
        sourceNode.connect(inverterNode);
        inverterNode.connect(gainNode);
        gainNode.connect(audioContext.destination);
      } finally {
        URL.revokeObjectURL(blobUrl);
      }
    } else {
      inverterNode = createScriptProcessorInverter(audioContext, 1024);
      sourceNode.connect(inverterNode);
      inverterNode.connect(gainNode);
      gainNode.connect(audioContext.destination);
      log("ScriptProcessor inverter running (high latency).");
    }

    // Update UI
    startBtn.disabled = true;
    stopBtn.disabled = false;
    running = true;

    // Show estimated output latency from context (if available)
    const estLatency = (audioContext.baseLatency || 0) * 1000;
    latencyEl.textContent = estLatency ? estLatency.toFixed(1) + ' ms' : 'unknown';

    log("Started. Use headphones and keep volume low initially.");
  }

  function stop() {
    if (!running) return;
    log("Stopping...");
    try {
      if (sourceNode) sourceNode.disconnect();
      if (inverterNode) inverterNode.disconnect?.();
      if (gainNode) gainNode.disconnect?.();
      if (micStream) {
        micStream.getTracks().forEach(t => t.stop());
        micStream = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
    } catch (err) {
      log("Error during stop:", err);
    }
    startBtn.disabled = false;
    stopBtn.disabled = true;
    running = false;
    log("Stopped.");
  }

  // Wire UI
  startBtn.addEventListener('click', async () => {
    await start();
  });
  stopBtn.addEventListener('click', stop);

  // Clean up when page hidden
  document.addEventListener('visibilitychange', () => {
    if (document.hidden) {
      // optional: pause/stop
    }
  });

  // Some browsers require user gesture to resume audio context
  window.addEventListener('click', () => {
    if (audioContext && audioContext.state === 'suspended') audioContext.resume();
  });

  // expose for debugging
  window._antiNoise = { start, stop };

  log("Ready. Click Start to begin.");
})();
</script>
</body>
</html>
