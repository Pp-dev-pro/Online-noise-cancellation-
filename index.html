 
  let audioContext = null;
  let micStream = null;
  let sourceNode = null;
  let inverterNode = null;
  let gainNode = null;
  let running = false;
  let useWorklet = !!(window.AudioWorkletNode && AudioWorkletProcessor);

  workletSupportEl.textContent = useWorklet ? "yes" : "no (will try ScriptProcessor fallback)";

  // AudioWorklet processor code as a string (we'll make a blob)
  const inverterWorkletCode = `
    class InverterProcessor extends AudioWorkletProcessor {
      static get parameterDescriptors() {
        return [
          { name: 'mix', defaultValue: 0.0, minValue:0, maxValue:1 }, // 0=pure anti-noise, 1=passthrough
          { name: 'gain', defaultValue: 1.0, minValue:0, maxValue:2 }
        ];
      }
      constructor(options) {
        super();
      }
      process (inputs, outputs, parameters) {
        const input = inputs[0];
        const output = outputs[0];
        if (!input || input.length === 0) {
          // silence if no input
          return true;
        }
        const mixArr = parameters.mix;
        const gainArr = parameters.gain;

        for (let channel = 0; channel < output.length; ++channel) {
          const inchan = input[channel] || input[0]; // fallback if mono
          const outchan = output[channel];
          if (!inchan) continue;

          for (let i = 0; i < outchan.length; ++i) {
            // parameter arrays are either length 1 (k-rate) or 128 (a-rate)
            const mix = mixArr.length > 1 ? mixArr[i] : mixArr[0];
            const gain = gainArr.length > 1 ? gainArr[i] : gainArr[0];

            // anti-noise: invert the input sample, scale by gain
            const anti = -inchan[i] * gain;

            // mix between anti-noise and passthrough input:
            outchan[i] = anti * (1 - mix) + inchan[i] * mix;
          }
        }
        return true;
      }
    }
    registerProcessor('inverter-processor', InverterProcessor);
  `;

  // Fallback ScriptProcessor implementation (higher latency)
  function createScriptProcessorInverter(context, bufferSize = 1024) {
    const sp = context.createScriptProcessor(bufferSize, 1, 1);
    sp.onaudioprocess = (evt) => {
      const inbuf = evt.inputBuffer;
      const outbuf = evt.outputBuffer;
      const inData = inbuf.getChannelData(0);
      const outData = outbuf.getChannelData(0);
      const gain = parseFloat(gainSlider.value);
      const mix = parseFloat(mixSlider.value);
      for (let i = 0; i < inData.length; ++i) {
        const anti = -inData[i] * gain;
        outData[i] = anti * (1 - mix) + inData[i] * mix;
      }
    };
    return sp;
  }

  async function start() {
    if (running) return;
    log("Starting...");
    try {
      // Request microphone with raw audio (disable browser processing)
      micStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 44100,
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false
        }
      });
    } catch (err) {
      log("getUserMedia error:", err.message || err);
      return;
    }

    // Create/resume AudioContext
    audioContext = new (window.AudioContext || window.webkitAudioContext)({ latencyHint: 'interactive' });
    await audioContext.audioWorklet?.readyCatch?.(); // no-op but nice for some browsers
    sourceNode = audioContext.createMediaStreamSource(micStream);

    // Create gain node (master output)
    gainNode = audioContext.createGain();
    gainNode.gain.value = parseFloat(gainSlider.value);

    if (useWorklet) {
      // Register worklet
      const blob = new Blob([inverterWorkletCode], { type: 'application/javascript' });
      const blobUrl = URL.createObjectURL(blob);
      try {
        await audioContext.audioWorklet.addModule(blobUrl);
        inverterNode = new AudioWorkletNode(audioContext, 'inverter-processor', {
          numberOfInputs: 1,
          numberOfOutputs: 1,
          channelCount: 1,
          parameterData: { mix: parseFloat(mixSlider.value), gain: parseFloat(gainSlider.value) }
        });

        // Keep parameters in sync with sliders
        mixSlider.addEventListener('input', () => {
          inverterNode.parameters.get('mix').setValueAtTime(parseFloat(mixSlider.value), audioContext.currentTime);
        });
        gainSlider.addEventListener('input', () => {
          inverterNode.parameters.get('gain').setValueAtTime(parseFloat(gainSlider.value), audioContext.currentTime);
          gainNode.gain.setValueAtTime(parseFloat(gainSlider.value), audioContext.currentTime);
        });

        sourceNode.connect(inverterNode);
        inverterNode.connect(gainNode);
        gainNode.connect(audioContext.destination);

        log("AudioWorklet inverter node running.");
      } catch (err) {
        log("AudioWorklet failed â€” falling back to ScriptProcessor:", err.message || err);
        inverterNode = createScriptProcessorInverter(audioContext, 1024);
        sourceNode.connect(inverterNode);
        inverterNode.connect(gainNode);
        gainNode.connect(audioContext.destination);
      } finally {
        URL.revokeObjectURL(blobUrl);
      }
    } else {
      inverterNode = createScriptProcessorInverter(audioContext, 1024);
      sourceNode.connect(inverterNode);
      inverterNode.connect(gainNode);
      gainNode.connect(audioContext.destination);
      log("ScriptProcessor inverter running (high latency).");
    }

    // Update UI
    startBtn.disabled = true;
    stopBtn.disabled = false;
    running = true;

    // Show estimated output latency from context (if available)
    const estLatency = (audioContext.baseLatency || 0) * 1000;
    latencyEl.textContent = estLatency ? estLatency.toFixed(1) + ' ms' : 'unknown';

    log("Started. Use headphones and keep volume low initially.");
  }

  function stop() {
    if (!running) return;
    log("Stopping...");
    try {
      if (sourceNode) sourceNode.disconnect();
      if (inverterNode) inverterNode.disconnect?.();
      if (gainNode) gainNode.disconnect?.();
      if (micStream) {
        micStream.getTracks().forEach(t => t.stop());
        micStream = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
    } catch (err) {
      log("Error during stop:", err);
    }
    startBtn.disabled = false;
    stopBtn.disabled = true;
    running = false;
    log("Stopped.");
  }

  // Wire UI
  startBtn.addEventListener('click', async () => {
    await start();
  });
  stopBtn.addEventListener('click', stop);

  // Clean up when page hidden
  document.addEventListener('visibilitychange', () => {
    if (document.hidden) {
      // optional: pause/stop
    }
  });

  // Some browsers require user gesture to resume audio context
  window.addEventListener('click', () => {
    if (audioContext && audioContext.state === 'suspended') audioContext.resume();
  });

  // expose for debugging
  window._antiNoise = { start, stop };

  log("Ready. Click Start to begin.");
})();
</script>
</body>
</html>
